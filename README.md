# ğŸ“‰ Customer Churn Prediction â€“ E-commerce App Users â€“ Python & scikit-learn  

**Author:** Loc Ha  
**Date:** 2025 August  

---

## ğŸ›  Tools Used  
![Python](https://img.shields.io/badge/Code-Python-blue)  
![Pandas](https://img.shields.io/badge/Library-Pandas-yellow)  
![scikit-learn](https://img.shields.io/badge/Library-scikit--learn-orange)  
![Matplotlib](https://img.shields.io/badge/Library-Matplotlib-green)  
![Seaborn](https://img.shields.io/badge/Library-Seaborn-red)  

---

## ğŸ“‘ Table of Contents  
1. [ğŸ“Œ Business Context & Objective](#1-business-context--objective)  
2. [ğŸ“‚ Dataset Description & Structure](#2-dataset-description--structure)  
3. [âš’ï¸ Main Process](#3-main-process)  
4. [ğŸ“Š Key Insights & Recommendations](#4-key-insights--recommendations)  

---
## 1. ğŸ“Œ Business Context & Objective  

### ğŸ¢ Business Question  
How can we **predict and understand churn behavior** among e-commerce app users to design effective retention strategies?  

### ğŸ¯ Objective  
- Identify behavioral patterns of churned users.  
- Build a **Machine Learning model** to predict churn with high recall/precision balance.  
- Segment churned users into groups for **personalized marketing campaigns**.  

---

## 2. ğŸ“‚ Dataset Description & Structure  

- **Source**: E-commerce churn dataset (simulated).  
- **Size**: 5,630 rows Ã— 20 columns  
- **Target column**: `Churn` (0 = Active, 1 = Churned)  
- **Missing values**: present in Tenure, DaySinceLastOrder, WarehouseToHome, CouponUsed, etc. (~200â€“300 each).  
- **ID column**: `CustomerID`  

---

## 3. âš’ï¸ Main Process

<details>
<summary>ğŸ“Œ Code Cell 1</summary>

```python
#Load file vÃ  phÃ¢n tÃ­ch EDA
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import chi2_contingency, ttest_ind

file_id = "1yxgr0Qj3TiXRehYa0PED1t4zIga9gdY5"
url = f"https://docs.google.com/spreadsheets/d/{file_id}/export?format=csv"

df = pd.read_csv(url)
print(df.head())
print(df.columns)
df.info()
df.describe()
df['Churn'].value_counts(normalize=True)
print(df.isnull().sum())
```  
</details>

*Placeholder for chart/output if applicable*  


<details>
<summary>ğŸ“Œ Code Cell 2</summary>

```python
#PhÃ¢n tÃ­ch cÃ¡c biáº¿n sá»‘ há»c - Numberic features - Correlation

numeric_cols = ['Tenure','SatisfactionScore','DaySinceLastOrder',
                'OrderCount','CouponUsed','CashbackAmount','HourSpendOnApp']

corrs = {}
for col in numeric_cols:
    corrs[col] = df[col].corr(df['Churn'])  # Pearson correlation (0/1 vá»›i numeric)

print("Correlation vá»›i Churn:")
for k,v in corrs.items():
    print(f"{k}: {v:.3f}")
```  
</details>

*Placeholder for chart/output if applicable*  


<details>
<summary>ğŸ“Œ Code Cell 3</summary>

```python
#PhÃ¢n tÃ­ch cÃ¡c biáº¿n sá»‘ há»c - Numberic features - Visualization
corr_df = pd.DataFrame.from_dict(corrs, orient='index', columns=['Correlation']).sort_values(by='Correlation')

# Plot bar chart
plt.figure(figsize=(8,5))
sns.barplot(x=corr_df.index, y='Correlation', data=corr_df, palette="coolwarm")
plt.xticks(rotation=45)
plt.title("Point Biserial Correlation between Churn and Numeric Features")
plt.axhline(0, color='black', linestyle='--')
plt.show()
```  
</details>

*Placeholder for chart/output if applicable*  


<details>
<summary>ğŸ“Œ Code Cell 4</summary>

```python
#AFTER FEEDBACK
#-------------------------

# PhÃ¢n tÃ­ch vá»›i biáº¿n phÃ¢n loáº¡i (Categorical features) - Chi-square test

cat_cols = ['PreferredLoginDevice','PreferredPaymentMode','Gender',
            'MaritalStatus','PreferedOrderCat','Complain']

# 1. Chuáº©n hÃ³a text trong cÃ¡c cá»™t phÃ¢n loáº¡i - Standadize text
for col in cat_cols:
    df[col] = df[col].astype(str).str.strip().str.title()  # Ä‘á»“ng nháº¥t viáº¿t hoa chá»¯ cÃ¡i Ä‘áº§u

# 2. Mapping thá»§ cÃ´ng náº¿u cÃ³ giÃ¡ trá»‹ cáº§n gá»™p - Mapping values manually
replace_dict = {
    'PreferredLoginDevice': {
        'Mobile Phone': 'Phone',
        'Phone': 'Phone'
    },
    'PreferredPaymentMode': {
        'Debit Card': 'Card',
        'Credit Card': 'Card',
        'Cc': 'Card',
        'Cash On Delivery':'COD',
        'Cod':'COD'
    },
}

df = df.replace(replace_dict) #Replace gom vá» 1 giÃ¡ trá»‹ - Replace and standadize values

# 3. PhÃ¢n tÃ­ch vÃ  trá»±c quan hÃ³a
for col in cat_cols:

    # Kiá»ƒm Ä‘á»‹nh Chi-square
    crosstab = pd.crosstab(df[col], df['Churn'])
    chi2, p, dof, ex = chi2_contingency(crosstab)

    # TÃ­nh churn rate + sá»‘ lÆ°á»£ng
    summary = df.groupby(col)['Churn'].agg(['mean','count','sum'])
    summary = summary.rename(columns={'mean':'ChurnRate','count':'Total','sum':'Churned'})
    summary = summary.sort_values(by='ChurnRate', ascending=False)

    # In báº£ng káº¿t quáº£
    print(f"\n=== {col} ===")
    print(summary.round(3))
    print(f"Chi-square test p-value = {p:.6f}")

    # Váº½ chart
    plt.figure(figsize=(6,4))
    sns.barplot(x=summary.index, y=summary['ChurnRate'], palette="viridis")
    plt.title(f"Churn rate by {col}")
    plt.ylabel("Churn rate")
    plt.xticks(rotation=45)
    plt.show()
```  
</details>

*Placeholder for chart/output if applicable*  


<details>
<summary>ğŸ“Œ Code Cell 5</summary>

```python
#Build the Machine Learning model for predicting churned users. (fine tuning)
```  
</details>

*Placeholder for chart/output if applicable*  


<details>
<summary>ğŸ“Œ Code Cell 6</summary>

```python
# Setup & Ä‘á»c dá»¯ liá»‡u

import numpy as np
import pandas as pd

from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from sklearn.metrics import (classification_report, roc_auc_score, average_precision_score,
                             roc_curve, precision_recall_curve, confusion_matrix)

from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
import xgboost as xgb
import joblib
import matplotlib.pyplot as plt
```  
</details>

*Placeholder for chart/output if applicable*  


<details>
<summary>ğŸ“Œ Code Cell 7</summary>

```python
# Chuáº©n hÃ³a tÃªn cá»™t
df.columns = df.columns.str.strip()

# Chuáº©n hÃ³a target Churn vá» 0/1 - Target & features
if df["Churn"].dtype == object:
    df["Churn"] = (df["Churn"].astype(str).str.strip()
                   .str.lower()
                   .map({"yes":1, "1":1, "true":1, "no":0, "0":0, "false":0})).astype(int)

# TÃ¡ch X, y - Separate features and target
y = df["Churn"].astype(int)
X = df.drop(columns=["Churn"])

# PhÃ¢n loáº¡i feature type - Column types
num_cols = X.select_dtypes(include=[np.number]).columns.tolist()
cat_cols = X.select_dtypes(include=["object", "category", "bool"]).columns.tolist()

print("Sá»‘ máº«u:", len(df))
print("Tá»· lá»‡ churn:", y.mean().round(4))
print("Sá»‘ numeric:", len(num_cols), "| Sá»‘ categorical:", len(cat_cols))
```  
</details>

*Placeholder for chart/output if applicable*  


<details>
<summary>ğŸ“Œ Code Cell 8</summary>

```python
#Pre-processing - tiá»n xá»­ lÃ½

num_tf = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

cat_tf = Pipeline([
    ("imputer", SimpleImputer(strategy="most_frequent")),
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

preprocessor = ColumnTransformer([
    ("num", num_tf, num_cols),
    ("cat", cat_tf, cat_cols)
])
```  
</details>

*Placeholder for chart/output if applicable*  


<details>
<summary>ğŸ“Œ Code Cell 9</summary>

```python
#Train, Valid, Split

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)
pos = y_train.sum(); neg = len(y_train) - pos
scale_pos_weight = (neg / pos) if pos > 0 else 1.0
scale_pos_weight
```  
</details>

*Placeholder for chart/output if applicable*  


<details>
<summary>ğŸ“Œ Code Cell 10</summary>

```python
#AFTER FEEDBACK
#-------------------------

# Baseline Models (chÆ°a tuning)
from sklearn.metrics import roc_auc_score, average_precision_score

# Logistic Regression (baseline)
pipe_lr = Pipeline([
    ("prep", preprocessor),
    ("clf", LogisticRegression(max_iter=2000, class_weight="balanced", solver="lbfgs"))
])
pipe_lr.fit(X_train, y_train)

y_pred_lr = pipe_lr.predict(X_test)
y_proba_lr = pipe_lr.predict_proba(X_test)[:,1]

print("Logistic Regression (Baseline)")
print("ROC-AUC:", roc_auc_score(y_test, y_proba_lr).round(4))
print("PR-AUC:", average_precision_score(y_test, y_proba_lr).round(4))

# Random Forest (baseline)
pipe_rf = Pipeline([
    ("prep", preprocessor),
    ("clf", RandomForestClassifier(class_weight="balanced", n_jobs=-1, random_state=42))
])
pipe_rf.fit(X_train, y_train)

y_pred_rf = pipe_rf.predict(X_test)
y_proba_rf = pipe_rf.predict_proba(X_test)[:,1]

print("Random Forest (Baseline)")
print("ROC-AUC:", roc_auc_score(y_test, y_proba_rf).round(4))
print("PR-AUC:", average_precision_score(y_test, y_proba_rf).round(4))
```  
</details>

*Placeholder for chart/output if applicable*  


<details>
<summary>ğŸ“Œ Code Cell 11</summary>

```python
## Sau khi xem káº¿t quáº£ baseline, thÃ¬ Random Forest tá»‘t hÆ¡n
# chá»n Random Forest model Ä‘á»ƒ tuning
```  
</details>

*Placeholder for chart/output if applicable*  


<details>
<summary>ğŸ“Œ Code Cell 12</summary>

```python
from sklearn.metrics import precision_recall_curve, classification_report, confusion_matrix, roc_auc_score, average_precision_score
import numpy as np

# XÃ¡c suáº¥t dá»± Ä‘oÃ¡n churn tá»« mÃ´ hÃ¬nh RF
y_proba = pipe_rf.predict_proba(X_test)[:,1]   # best_model = RF Ä‘Ã£ fit
y_true = y_test

# Precision-Recall curve
precisions, recalls, thresholds = precision_recall_curve(y_true, y_proba)
thresholds = np.append(thresholds, 1.0)  # khÃ©p kÃ­n 1.0

# TÃ­nh F1 cho tá»«ng threshold
f1s = 2 * (precisions * recalls) / (precisions + recalls + 1e-12)
idx_f1 = np.nanargmax(f1s)

thr_f1 = thresholds[idx_f1]
print(f"NgÆ°á»¡ng tá»‘i Æ°u theo F1 = {thr_f1:.3f} | Precision={precisions[idx_f1]:.3f} | Recall={recalls[idx_f1]:.3f}")

# Chá»n ngÆ°á»¡ng Ä‘á»ƒ Ä‘áº¡t Recall â‰¥ 0.80
target_recall = 0.80
mask = recalls >= target_recall
if mask.any():
    idx_rec = np.argmax(precisions[mask])  # chá»n precision cao nháº¥t trong sá»‘ recall â‰¥ 0.8
    idx_rec = np.where(mask)[0][idx_rec]
    thr_rec = thresholds[idx_rec]
    print(f"NgÆ°á»¡ng Ä‘áº¡t Recall â‰¥ {target_recall}: {thr_rec:.3f} | Precision={precisions[idx_rec]:.3f} | Recall={recalls[idx_rec]:.3f}")
else:
    print("KhÃ´ng Ä‘áº¡t Ä‘Æ°á»£c Recall â‰¥ 0.80 vá»›i báº¥t ká»³ ngÆ°á»¡ng nÃ o.")

# ÄÃ¡nh giÃ¡ confusion matrix táº¡i threshold tá»‘i Æ°u F1
y_pred_f1 = (y_proba >= thr_f1).astype(int)
print("\n=== Káº¿t quáº£ vá»›i threshold tá»‘i Æ°u F1 ===")
print(confusion_matrix(y_true, y_pred_f1))
print(classification_report(y_true, y_pred_f1, digits=3))

# ÄÃ¡nh giÃ¡ táº¡i threshold Recallâ‰¥0.80
if mask.any():
    y_pred_rec = (y_proba >= thr_rec).astype(int)
    print("\n=== Káº¿t quáº£ vá»›i threshold Recallâ‰¥0.80 ===")
    print(confusion_matrix(y_true, y_pred_rec))
    print(classification_report(y_true, y_pred_rec, digits=3))
```  
</details>

*Placeholder for chart/output if applicable*  


<details>
<summary>ğŸ“Œ Code Cell 13</summary>

```python
# LÆ°u káº¿t quáº£
results = pd.DataFrame({
    "CustomerID": X_test["CustomerID"].values if "CustomerID" in X_test.columns else range(len(X_test)),
    "y_true": y_test.values,
    "y_proba": y_proba,
    "y_pred_F1": y_pred_f1,
    "y_pred_Recall80": (y_proba >= 0.490).astype(int)  # Ä‘á»•i 0.490 náº¿u báº¡n chá»n ngÆ°á»¡ng khÃ¡c
})
results.head()
```  
</details>

*Placeholder for chart/output if applicable*  


<details>
<summary>ğŸ“Œ Code Cell 14</summary>

```python
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Lá»c churn users
churn_users = df[df["Churn"]==1].copy()

# Chá»n má»™t sá»‘ biáº¿n hÃ nh vi quan trá»ng Ä‘á»ƒ phÃ¢n cá»¥m
features = ["Tenure","DaySinceLastOrder","OrderCount","CouponUsed",
            "CashbackAmount","SatisfactionScore","HourSpendOnApp"]

# Xá»­ lÃ½ missing náº¿u cÃ³
X_cluster = churn_users[features].fillna(0)
X_scaled = StandardScaler().fit_transform(X_cluster)

# Chá»n sá»‘ cluster báº±ng Elbow method
wcss = []
for k in range(2,7):
    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
    kmeans.fit(X_scaled)
    wcss.append(kmeans.inertia_)

plt.plot(range(2,7), wcss, marker="o")
plt.xlabel("Number of clusters")
plt.ylabel("WCSS")
plt.title("Elbow Method")
plt.show()
```  
</details>

*Placeholder for chart/output if applicable*  


<details>
<summary>ğŸ“Œ Code Cell 15</summary>

```python
# Chá»n k=4
kmeans = KMeans(n_clusters=4, random_state=42, n_init=10)
churn_users["Cluster"] = kmeans.fit_predict(X_scaled)

# Xem Ä‘áº·c trÆ°ng trung bÃ¬nh má»—i nhÃ³m
cluster_summary = churn_users.groupby("Cluster")[features].mean().round(2)
print(cluster_summary)
```  
</details>

*Placeholder for chart/output if applicable*  



---

## 4. ğŸ“Š Key Insights & Recommendations  

### ğŸ’¡ Insights from EDA  
- Tenure â†“ â†’ churn â†‘ (new users at risk).  
- Inactive days â†‘ â†’ churn â†‘ (reactivation campaigns needed).  
- Cashback â†‘ â†’ churn â†“ (effective lever).  
- COD payment & complaints strongly linked to churn.  
- iPhone/computer users churn more.  

---

### ğŸ” Segmentation Results  
- **Cluster 0**: Long-tenure, cashback â†’ loyalty rewards.  
- **Cluster 1**: High orders + low satisfaction â†’ service improvement.  
- **Cluster 2**: New & engaged users â†’ welcome offers.  
- **Cluster 3**: Very new, low activity â†’ onboarding campaigns.  

---

### ğŸ“ Recommendations  
1. ğŸ¯ Focus on **new user retention** via onboarding & welcome offers.  
2. ğŸ’³ Promote **digital payments** to reduce COD churn.  
3. ğŸ Strengthen **cashback & loyalty** for long-tenure users.  
4. âš¡ Reactivate inactive users with **personalized promotions**.  
5. ğŸ›  Improve **customer service** to reduce churn from complaints.  
6. ğŸ‘¥ Deploy **cluster-based marketing** for targeted retention.  
